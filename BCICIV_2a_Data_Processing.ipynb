{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BCICIV_2a Data Processing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BCI-and-Neuroergonomics-Lab/3D-MB-CNN/blob/dev/BCICIV_2a_Data_Processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toxVGk3hLTws"
      },
      "source": [
        "Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPxdtyc_LUWg",
        "outputId": "3f8da1aa-a958-4a97-c73c-3d3afbeffa5a"
      },
      "source": [
        "# Loading your drive so CoLab has access to it\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEEriEuYLG7C"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "du1xZaGyBgN6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a61e63b0-c46d-4411-edbf-bc53849572c7"
      },
      "source": [
        "import os, sys, re\n",
        "from scipy.io import loadmat\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "import warnings\n",
        "try:\n",
        "  import mne\n",
        "except ModuleNotFoundError:\n",
        "  !pip install mne\n",
        "  import mne"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mne\n",
            "  Downloading mne-1.0.0-py3-none-any.whl (7.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.5 MB 6.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pooch>=1.5 in /usr/local/lib/python3.7/dist-packages (from mne) (1.6.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from mne) (4.4.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from mne) (4.63.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from mne) (21.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from mne) (2.11.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from mne) (3.2.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from mne) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from mne) (1.21.5)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.5->mne) (2.23.0)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.5->mne) (1.4.4)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->mne) (3.0.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (1.24.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->mne) (2.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mne) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mne) (1.4.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mne) (0.11.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->mne) (3.10.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->mne) (1.15.0)\n",
            "Installing collected packages: mne\n",
            "Successfully installed mne-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Handle Warnings"
      ],
      "metadata": {
        "id": "7ep6wRglOLEK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
        "warnings.simplefilter(action='ignore', category=RuntimeWarning)"
      ],
      "metadata": {
        "id": "ZkehPVcIOMf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGUVAcJWLcbx"
      },
      "source": [
        "Constants\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4-7chSuLd0q"
      },
      "source": [
        "# Data Processing Mode ('3D' or '2D')\n",
        "\n",
        "MODE = '3D'\n",
        "\n",
        "# File Patterns for Data Files\n",
        "\n",
        "DATA_FILE_PATTERN = \"/A0{}{}.gdf\"\n",
        "KEY_FILE_PATTERN = \"/A0{}{}.mat\"\n",
        "SUFFIX_3D_DIRS = \"_3d/\"\n",
        "SUFFIX_2D_DIRS = \"_2d/\"\n",
        "\n",
        "# Data Directories\n",
        "DEFAULT_DIR = \"/content/drive/MyDrive/BCI-Lab-Drive/Datasets/BCICIV_2a/BCICIV_2a_raw\"\n",
        "\n",
        "KEY_DIR = \"/content/drive/MyDrive/BCI-Lab-Drive/Datasets/BCICIV_2a/BCICIV_2a_raw/true_labels\"\n",
        "\n",
        "PROCESSED_DIR = \"/content/drive/MyDrive/BCI-Lab-Drive/Datasets/BCICIV_2a/BCICIV_2a_processed\"\n",
        "\n",
        "CROPPED_DIR = \"/content/drive/MyDrive/BCI-Lab-Drive/Datasets/BCICIV_2a/BCICIV_2a_cropped\"\n",
        "\n",
        "AVERAGED_DIR = \"/content/drive/MyDrive/BCI-Lab-Drive/Datasets/BCICIV_2a/BCICIV_2a_averaged\"\n",
        "\n",
        "SHUFFLED_DIR = \"/content/drive/MyDrive/BCI-Lab-Drive/Datasets/BCICIV_2a/BCICIV_2a_shuffled\"\n",
        "\n",
        "# Data Split\n",
        "\n",
        "TRAIN_DIR_APPEND = \"train/\"\n",
        "\n",
        "VAL_DIR_APPEND = \"val/\"\n",
        "\n",
        "TEST_DIR_APPEND = \"test/\"\n",
        "\n",
        "SPLIT_TYPES = ('train', 'test', 'val')\n",
        "SPLIT_DIR_DICT = {'train': TRAIN_DIR_APPEND, 'test': TEST_DIR_APPEND, 'val': VAL_DIR_APPEND}\n",
        "\n",
        "# Split Ratios, results in a 60/20/20 split\n",
        "# First data is split between training and val/test\n",
        "# Second val/test is split into val and test\n",
        "TRAIN_SPLIT = 0.6\n",
        "TEST_SPLIT = 0.5\n",
        "\n",
        "SHUFFLE_SPLITS = True\n",
        "\n",
        "# Data Processing Parameters\n",
        "\n",
        "# Event Tags\n",
        "# '783' corresponds to the eval set events, since they are not keyed\n",
        "EVENT_TAGS = ('769', '770', '771', '772', '783')\n",
        "\n",
        "# Event Tag Meanings\n",
        "# EVENT_DICT = {'769': 'Left', '770': 'Right', '771': 'Foot', '772': 'Tongue'}\n",
        "EVENT_DICT = {'769': 0, '770': 1, '771': 2, '772': 3, '783': 4}\n",
        "\n",
        "BADS = ['EOG-left', 'EOG-central', 'EOG-right']\n",
        "\n",
        "NUM_CHANNELS = 22\n",
        "\n",
        "T_AFTER_STIM = 0.5\n",
        "\n",
        "# Initial time window extracted from raw data\n",
        "WINDOW = 2\n",
        "# Data frequency\n",
        "FREQ = 250\n",
        "# Lower Band of Filter\n",
        "LOW_BAND_FREQ = 4\n",
        "# Higher Band of Filter\n",
        "HIGH_BAND_FREQ = 40\n",
        "# Number of classes\n",
        "NUM_CLASSES = 4\n",
        "# Number of subjects\n",
        "NUM_SUBJECTS = 9\n",
        "# X dim of 3D data\n",
        "X_DIM_3D = 7\n",
        "# Y dim of 3D data\n",
        "Y_DIM_3D = 6\n",
        "# Window size post cropping\n",
        "WINDOW_SIZE = 313\n",
        "# Step size while cropping\n",
        "STEP_SIZE = 20\n",
        "# Which channels are included, matched with BCICIV_2a dataset channels\n",
        "INCLUDE = ('EEG-Fz', 'EEG-0', 'EEG-1', 'EEG-2', 'EEG-3', 'EEG-4', 'EEG-5', 'EEG-C3', 'EEG-6', 'EEG-Cz', 'EEG-7', 'EEG-C4', 'EEG-8', 'EEG-9', 'EEG-10', 'EEG-11', 'EEG-12', 'EEG-13', 'EEG-14', 'EEG-Pz', 'EEG-15', 'EEG-16')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jk1RYyTXMCLg"
      },
      "source": [
        "Data Processing Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3jheDD2MBu-"
      },
      "source": [
        "class DataProcessing:\n",
        "  def __init__(self, data_dir=DEFAULT_DIR, event_tags = EVENT_TAGS, mode='3D'):\n",
        "    if (mode != '3D' and mode != '2D'):\n",
        "      raise Exception(\"Invalid Data Dimension, please use '3D' or '2D'.\")\n",
        "    self.dim = mode\n",
        "    self.event_tags = event_tags\n",
        "    self.data_files = {}\n",
        "    self.data_file_counts = {}\n",
        "    train_count = 0\n",
        "    eval_count = 0\n",
        "    for subject in range(1, NUM_SUBJECTS+1):\n",
        "      self.data_files[subject] = {}\n",
        "      for train_or_eval in 'TE':\n",
        "        data_file_name = data_dir + DATA_FILE_PATTERN.format(subject, train_or_eval)\n",
        "        if (os.path.exists(data_file_name)):\n",
        "          self.data_files[subject][train_or_eval] = data_file_name\n",
        "          if (train_or_eval == 'T'):\n",
        "            train_count += 1\n",
        "          if (train_or_eval == 'E'):\n",
        "            eval_count += 1\n",
        "        else:\n",
        "          raise Exception(\"Failed to find data file, verify files are located in data directory.\")\n",
        "    print(\"Found {} Training Files\".format(train_count))\n",
        "    print(\"Found {} Evaluation Files\".format(eval_count))\n",
        "\n",
        "  def __convertIndices(self, channel):\n",
        "    xDict = {'EEG-Fz':3, 'EEG-0':1, 'EEG-1':2, 'EEG-2':3, 'EEG-3':4, 'EEG-4':5, 'EEG-5':0, 'EEG-C3':1, 'EEG-6':2, 'EEG-Cz':3, 'EEG-7':4, 'EEG-C4':5, 'EEG-8':6, 'EEG-9':1, 'EEG-10':2, 'EEG-11':3, 'EEG-12':4, 'EEG-13':5, 'EEG-14':2, 'EEG-Pz':3, 'EEG-15':4, 'EEG-16':3}\n",
        "    yDict = {'EEG-Fz':0, 'EEG-0':1, 'EEG-1':1, 'EEG-2':1, 'EEG-3':1, 'EEG-4':1, 'EEG-5':2, 'EEG-C3':2, 'EEG-6':2, 'EEG-Cz':2, 'EEG-7':2, 'EEG-C4':2, 'EEG-8':2, 'EEG-9':3, 'EEG-10':3, 'EEG-11':3, 'EEG-12':3, 'EEG-13':3, 'EEG-14':4, 'EEG-Pz':4, 'EEG-15':4, 'EEG-16':5}\n",
        "    return xDict[channel], yDict[channel]\n",
        "\n",
        "  def __liveProgress(self, tag, current, max):\n",
        "    output = '\\r' + tag + \" - [\"\n",
        "    for i in range(current):\n",
        "      output += '='\n",
        "    for i in range(max - current):\n",
        "      output += '-'\n",
        "    print(output + \"] - {}/{} - {:.2%}\".format(current, max, current/max), end='')\n",
        "\n",
        "  def processData(self):\n",
        "    self.processed_data = {'train': {}, 'test': {}, 'val': {}}\n",
        "    if (self.dim == '3D'):\n",
        "      data_shape = [0, X_DIM_3D, Y_DIM_3D, int(WINDOW*FREQ)]\n",
        "    elif (self.dim == '2D'):\n",
        "      data_shape = [0, NUM_CHANNELS, int(WINDOW*FREQ)]\n",
        "    else:\n",
        "      raise Exception(\"Invalid Data Dimension, please use '3D' or '2D'.\")\n",
        "    key_shape =[0, NUM_CLASSES]\n",
        "    \n",
        "    tmin = T_AFTER_STIM\n",
        "    tmax = tmin + WINDOW - (1/FREQ)\n",
        "\n",
        "    for subject in range(1, NUM_SUBJECTS+1):\n",
        "      self.processed_data['train'][subject] = {}\n",
        "      self.processed_data['test'][subject] = {}\n",
        "      self.processed_data['val'][subject] = {}\n",
        "\n",
        "      training_data = None\n",
        "      training_keys = None\n",
        "      eval_data = None\n",
        "      eval_keys = None\n",
        "\n",
        "      for num, train_or_eval in enumerate('TE'):\n",
        "        self.__liveProgress(\"Processing Data\", ((subject-1)*2)+num+1, NUM_SUBJECTS*2)\n",
        "\n",
        "        # This code will need revisiting, but want to get it working for now\n",
        "        raw = mne.io.read_raw_gdf(self.data_files[subject][train_or_eval], preload=True, verbose=False)\n",
        "        raw.filter(LOW_BAND_FREQ, HIGH_BAND_FREQ)\n",
        "        events, temp_id = mne.events_from_annotations(raw, verbose=False)\n",
        "        event_id = {}\n",
        "        conversion_dict = {}\n",
        "        for key, value in temp_id.items():\n",
        "          if (key in self.event_tags):\n",
        "            event_id[key] = value\n",
        "            conversion_dict[value] = EVENT_DICT[key]\n",
        "        raw.info['bads'] = BADS\n",
        "        picks = mne.pick_types(raw.info, meg=False, eeg=True, stim=False, eog=False, exclude='bads')\n",
        "        epochs = mne.Epochs(raw, events, event_id, tmin, tmax, proj=False, event_repeated='merge', picks=picks, baseline=None, preload=True, verbose=False)\n",
        "        num_trials = len(epochs)\n",
        "        data_shape[0] = num_trials\n",
        "        data = np.zeros(data_shape)\n",
        "        epochs.reorder_channels(INCLUDE)\n",
        "        raw_data = epochs.get_data()\n",
        "        if (self.dim == '3D'):\n",
        "          for (i, channel) in enumerate(INCLUDE):\n",
        "            x, y = self.__convertIndices(channel)\n",
        "            data[:, x, y, :] = raw_data[:, i, :]\n",
        "        elif (self.dim == '2D'):\n",
        "          data = raw_data\n",
        "        else:\n",
        "          raise Exception(\"Invalid Data Dimension, please use '3D' or '2D'.\")\n",
        "        key_shape[0] = num_trials\n",
        "        keys = np.zeros(key_shape)\n",
        "        file_name = KEY_DIR + KEY_FILE_PATTERN.format(subject, train_or_eval)\n",
        "        key_file = loadmat(file_name)\n",
        "        keys = key_file['classlabel'] - 1\n",
        "        keys_1 = np.max(keys) + 1\n",
        "        keys = np.eye(keys_1)[keys]\n",
        "        keys = np.squeeze(keys)\n",
        "\n",
        "        if (train_or_eval == 'T'):\n",
        "          training_data = data\n",
        "          training_keys = keys\n",
        "        elif (train_or_eval == 'E'):\n",
        "          eval_data = data\n",
        "          eval_keys = keys\n",
        "\n",
        "      all_data = np.concatenate((training_data, eval_data))\n",
        "      all_keys = np.concatenate((training_keys, eval_keys))\n",
        "\n",
        "      data_train, data_test_val, key_train, key_test_val = train_test_split(all_data, all_keys, train_size=TRAIN_SPLIT, shuffle=SHUFFLE_SPLITS, stratify=(all_keys if SHUFFLE_SPLITS else None))\n",
        "      data_test, data_val, key_test, key_val = train_test_split(data_test_val, key_test_val, train_size=TEST_SPLIT, shuffle=SHUFFLE_SPLITS, stratify=(key_test_val if SHUFFLE_SPLITS else None))\n",
        "\n",
        "      self.processed_data['train'][subject] = (data_train, key_train)\n",
        "      self.processed_data['test'][subject] = (data_test, key_test)\n",
        "      self.processed_data['val'][subject] = (data_val, key_val)\n",
        "\n",
        "    print(\"\\rFinished processing data\")\n",
        "\n",
        "  def saveProcessedData(self):\n",
        "    if (self.dim == '3D'):\n",
        "      fileBase = PROCESSED_DIR + SUFFIX_3D_DIRS\n",
        "    elif (self.dim == '2D'):\n",
        "      fileBase = PROCESSED_DIR + SUFFIX_2D_DIRS\n",
        "    else:\n",
        "      raise Exception(\"Invalid Data Dimension, please use '3D' or '2D'.\")\n",
        "    for split_num, split_type in enumerate(SPLIT_TYPES):\n",
        "      for subject in range(1, NUM_SUBJECTS+1):\n",
        "        self.__liveProgress(\"Saving Processed Data\", (split_num)*NUM_SUBJECTS + subject, NUM_SUBJECTS*len(SPLIT_TYPES))\n",
        "        np.save(fileBase + SPLIT_DIR_DICT[split_type] + \"A0\" + str(subject) + \"D_processed.npy\", self.processed_data[split_type][subject][0])\n",
        "        np.save(fileBase + SPLIT_DIR_DICT[split_type] + \"A0\" + str(subject) + \"K_processed.npy\", self.processed_data[split_type][subject][1])\n",
        "    print(\"\\rFinished saving processed data\")\n",
        "\n",
        "  def loadProcessedData(self, mode = None):\n",
        "    if (mode == None):\n",
        "      mode = self.dim\n",
        "    if (mode == '3D'):\n",
        "      fileBase = PROCESSED_DIR + SUFFIX_3D_DIRS\n",
        "    elif (mode == '2D'):\n",
        "      fileBase = PROCESSED_DIR + SUFFIX_2D_DIRS\n",
        "    else:\n",
        "      raise Exception(\"Invalid Data Dimension, please use '3D' or '2D'.\")\n",
        "    self.processed_data = {}\n",
        "    fileSuffD = \"D_processed.npy\"\n",
        "    fileSuffK = \"K_processed.npy\"\n",
        "    for split_num, split_type in enumerate(SPLIT_TYPES):\n",
        "      self.processed_data[split_type] = {}\n",
        "      for subject in range(1, NUM_SUBJECTS+1):\n",
        "        self.__liveProgress(\"Loading Processed Data\", (split_num)*NUM_SUBJECTS + subject, NUM_SUBJECTS*len(SPLIT_TYPES))\n",
        "        d = np.load(fileBase + SPLIT_DIR_DICT[split_type] + \"A0\" + str(subject) + fileSuffD)\n",
        "        k = np.load(fileBase + SPLIT_DIR_DICT[split_type] + \"A0\" + str(subject) + fileSuffK)\n",
        "        self.processed_data[split_type][subject] = (d, k)\n",
        "    print(\"\\rFinished loading processed data\")\n",
        "  \n",
        "  def cropData(self):\n",
        "    steps = ((int(WINDOW*FREQ) - WINDOW_SIZE) // STEP_SIZE) + 1\n",
        "    self.cropped_data = {}\n",
        "    for split_num, split_type in enumerate(SPLIT_TYPES):\n",
        "      self.cropped_data[split_type] = {}\n",
        "      for subject in range(1, NUM_SUBJECTS+1):\n",
        "        self.__liveProgress(\"Cropping Data\", (split_num)*NUM_SUBJECTS + subject, NUM_SUBJECTS*len(SPLIT_TYPES))\n",
        "        if (self.dim == '3D'):\n",
        "          data_shape = (int(steps*len(self.processed_data[split_type][subject][0])), X_DIM_3D, Y_DIM_3D, WINDOW_SIZE)\n",
        "        elif (self.dim == '2D'):\n",
        "          data_shape = (int(steps*len(self.processed_data[split_type][subject][0])), len(INCLUDE), WINDOW_SIZE)\n",
        "        else:\n",
        "          raise Exception(\"Invalid Data Dimension, please use '3D' or '2D'.\")\n",
        "        key_shape = (int(steps*len(self.processed_data[split_type][subject][1])), NUM_CLASSES)\n",
        "        d = np.zeros(data_shape)\n",
        "        for sample in range(len(self.processed_data[split_type][subject][0])):\n",
        "          for step in range(steps):\n",
        "            if (self.dim == '3D'):\n",
        "              d[int(sample*steps + step)] = self.processed_data[split_type][subject][0][sample][:, :, (step*STEP_SIZE):(step*STEP_SIZE)+WINDOW_SIZE]\n",
        "            elif (self.dim == '2D'):\n",
        "              d[int(sample*steps + step)] = self.processed_data[split_type][subject][0][sample][:, (step*STEP_SIZE):(step*STEP_SIZE)+WINDOW_SIZE]\n",
        "        k = np.zeros(key_shape)\n",
        "        for sample in range(len(self.processed_data[split_type][subject][1])):\n",
        "          for step in range(steps):\n",
        "            k[int(sample*steps + step)] = self.processed_data[split_type][subject][1][sample]\n",
        "        self.cropped_data[split_type][subject] = (d, k)\n",
        "    print(\"\\rFinished cropping data\")\n",
        "\n",
        "  def saveCroppedData(self):\n",
        "    if (self.dim == '3D'):\n",
        "      fileBase = CROPPED_DIR + SUFFIX_3D_DIRS\n",
        "    elif (self.dim == '2D'):\n",
        "      fileBase = CROPPED_DIR + SUFFIX_2D_DIRS\n",
        "    else:\n",
        "      raise Exception(\"Invalid Data Dimension, please use '3D' or '2D'.\")\n",
        "    for split_num, split_type in enumerate(SPLIT_TYPES):\n",
        "      for subject in range(1, NUM_SUBJECTS+1):\n",
        "        self.__liveProgress(\"Saving Cropped Data\", (split_num)*NUM_SUBJECTS + subject, NUM_SUBJECTS*len(SPLIT_TYPES))\n",
        "        np.save(fileBase + SPLIT_DIR_DICT[split_type] + \"A0\" + str(subject) + \"D_cropped.npy\", self.cropped_data[split_type][subject][0])\n",
        "        np.save(fileBase + SPLIT_DIR_DICT[split_type] + \"A0\" + str(subject) + \"K_cropped.npy\", self.cropped_data[split_type][subject][1])\n",
        "    print(\"\\rFinished saving cropped data\")\n",
        "  \n",
        "  def loadCroppedData(self, mode = None):\n",
        "    if (mode == None):\n",
        "      mode = self.dim\n",
        "    if (mode == '3D'):\n",
        "      fileBase = CROPPED_DIR + SUFFIX_3D_DIRS\n",
        "    elif (mode == '2D'):\n",
        "      fileBase = CROPPED_DIR + SUFFIX_2D_DIRS\n",
        "    else:\n",
        "      raise Exception(\"Invalid Data Dimension, please use '3D' or '2D'.\")\n",
        "    self.cropped_data = {}\n",
        "    fileSuffD = \"D_cropped.npy\"\n",
        "    fileSuffK = \"K_cropped.npy\"\n",
        "    for split_num, split_type in enumerate(SPLIT_TYPES):\n",
        "      self.cropped_data[split_type] = {}\n",
        "      for subject in range(1, NUM_SUBJECTS+1):\n",
        "        self.__liveProgress(\"Loading Cropped Data\", (split_num)*NUM_SUBJECTS + subject, NUM_SUBJECTS*len(SPLIT_TYPES))\n",
        "        d = np.load(fileBase + SPLIT_DIR_DICT[split_type] + \"A0\" + str(subject) + fileSuffD)\n",
        "        k = np.load(fileBase + SPLIT_DIR_DICT[split_type] + \"A0\" + str(subject) + fileSuffK)\n",
        "        self.cropped_data[split_type][subject] = (d, k)\n",
        "    print(\"\\rFinished loading cropped data\")\n",
        "  \n",
        "  def averageData(self):\n",
        "    self.averaged_data = {}\n",
        "    if (self.dim == '3D'):\n",
        "      data_shape = (X_DIM_3D, Y_DIM_3D)\n",
        "    elif (self.dim == '2D'):\n",
        "      data_shape = (len(INCLUDE))\n",
        "    else:\n",
        "      raise Exception(\"Invalid Data Dimension, please use '3D' or '2D'.\")\n",
        "    for split_num, split_type in enumerate(SPLIT_TYPES):\n",
        "      self.averaged_data[split_type] = {}\n",
        "      for subject in range(1, NUM_SUBJECTS+1):\n",
        "        self.__liveProgress(\"Averaging Data\", (split_num)*NUM_SUBJECTS + subject, NUM_SUBJECTS*len(SPLIT_TYPES))\n",
        "        self.averaged_data[split_type][subject] = (np.zeros(self.cropped_data[split_type][subject][0].shape), self.cropped_data[split_type][subject][1])\n",
        "        for sample in range(len(self.cropped_data[split_type][subject][0])):\n",
        "          a = np.zeros(data_shape)\n",
        "          if (self.dim == '3D'):\n",
        "            for x in range(X_DIM_3D):\n",
        "              for y in range(Y_DIM_3D):\n",
        "                a[x, y] = np.average(self.cropped_data[split_type][subject][0][sample][x, y])\n",
        "            a = np.repeat(a[:, :, np.newaxis], WINDOW_SIZE, axis=2)\n",
        "          elif (self.dim == '2D'):\n",
        "            for channel in range(len(INCLUDE)):\n",
        "              a[channel] = np.average(self.cropped_data[split_type][subject][0][sample][channel])\n",
        "            a = np.repeat(a[:, np.newaxis], WINDOW_SIZE, axis=1)\n",
        "          else:   \n",
        "            raise Exception(\"Invalid Data Dimension, please use '3D' or '2D'.\")\n",
        "          self.averaged_data[split_type][subject][0][sample] = self.cropped_data[split_type][subject][0][sample] - a\n",
        "    print(\"\\rFinished averaging data\")\n",
        "\n",
        "  def saveAveragedData(self):\n",
        "    if (self.dim == '3D'):\n",
        "      fileBase = AVERAGED_DIR + SUFFIX_3D_DIRS\n",
        "    elif (self.dim == '2D'):\n",
        "      fileBase = AVERAGED_DIR + SUFFIX_2D_DIRS\n",
        "    else:\n",
        "      raise Exception(\"Invalid Data Dimension, please use '3D' or '2D'.\")\n",
        "    for split_num, split_type in enumerate(SPLIT_TYPES):\n",
        "      for subject in range(1, NUM_SUBJECTS+1):\n",
        "        self.__liveProgress(\"Saving Averaged Data\", (split_num)*NUM_SUBJECTS + subject, NUM_SUBJECTS*len(SPLIT_TYPES))\n",
        "        np.save(fileBase + SPLIT_DIR_DICT[split_type] + \"A0\" + str(subject) + \"D_averaged.npy\", self.averaged_data[split_type][subject][0])\n",
        "        np.save(fileBase + SPLIT_DIR_DICT[split_type] + \"A0\" + str(subject) + \"K_averaged.npy\", self.averaged_data[split_type][subject][1])\n",
        "    print(\"\\rFinished saving averaged data\")\n",
        "\n",
        "  def loadAveragedData(self, mode = None):\n",
        "    if (mode == None):\n",
        "      mode = self.dim\n",
        "    if (mode == '3D'):\n",
        "      fileBase = AVERAGED_DIR + SUFFIX_3D_DIRS\n",
        "    elif (mode == '2D'):\n",
        "      fileBase = AVERAGED_DIR + SUFFIX_2D_DIRS\n",
        "    else:\n",
        "      raise Exception(\"Invalid Data Dimension, please use '3D' or '2D'.\")\n",
        "    self.averaged_data = {}\n",
        "    fileSuffD = \"D_averaged.npy\"\n",
        "    fileSuffK = \"K_averaged.npy\"\n",
        "    for split_num, split_type in enumerate(SPLIT_TYPES):\n",
        "      self.averaged_data[split_type] = {}\n",
        "      for subject in range(1, NUM_SUBJECTS+1):\n",
        "        self.__liveProgress(\"Loading Averaged Data\", (split_num)*NUM_SUBJECTS + subject, NUM_SUBJECTS*len(SPLIT_TYPES))\n",
        "        d = np.load(fileBase + SPLIT_DIR_DICT[split_type] + \"A0\" + str(subject) + fileSuffD)\n",
        "        k = np.load(fileBase + SPLIT_DIR_DICT[split_type] + \"A0\" + str(subject) + fileSuffK)\n",
        "        self.averaged_data[split_type][subject] = (d, k)\n",
        "    print(\"\\rFinished loading averaged data\")\n",
        "\n",
        "  def shuffleData(self):\n",
        "    self.shuffled_data = {}\n",
        "    for split_num, split_type in enumerate(SPLIT_TYPES):\n",
        "      self.shuffled_data[split_type] = {}\n",
        "      for subject in range(1, NUM_SUBJECTS+1):\n",
        "        self.__liveProgress(\"Shuffling Data\", (split_num)*NUM_SUBJECTS + subject, NUM_SUBJECTS*len(SPLIT_TYPES))\n",
        "        p = np.random.permutation(len(self.averaged_data[split_type][subject][0]))\n",
        "        self.shuffled_data[split_type][subject] = (self.averaged_data[split_type][subject][0][p], self.averaged_data[split_type][subject][1][p])\n",
        "    print(\"\\rFinished shuffling data\")\n",
        "  \n",
        "  def saveShuffledData(self):\n",
        "    if (self.dim == '3D'):\n",
        "      fileBase = SHUFFLED_DIR + SUFFIX_3D_DIRS\n",
        "    elif (self.dim == '2D'):\n",
        "      fileBase = SHUFFLED_DIR + SUFFIX_2D_DIRS\n",
        "    else:\n",
        "      raise Exception(\"Invalid Data Dimension, please use '3D' or '2D'.\")\n",
        "    for split_num, split_type in enumerate(SPLIT_TYPES):\n",
        "      for subject in range(1, NUM_SUBJECTS+1):\n",
        "        self.__liveProgress(\"Saving Shuffled Data\", (split_num)*NUM_SUBJECTS + subject, NUM_SUBJECTS*len(SPLIT_TYPES))\n",
        "        np.save(fileBase + SPLIT_DIR_DICT[split_type] + \"A0\" + str(subject) + \"D_shuffled.npy\", self.shuffled_data[split_type][subject][0])\n",
        "        np.save(fileBase + SPLIT_DIR_DICT[split_type] + \"A0\" + str(subject) + \"K_shuffled.npy\", self.shuffled_data[split_type][subject][1])\n",
        "    print(\"\\rFinished saving shuffled data\")\n",
        "\n",
        "  def loadShuffledData(self, mode = None):\n",
        "    if (mode == None):\n",
        "      mode = self.dim\n",
        "    if (mode == '3D'):\n",
        "      fileBase = SHUFFLED_DIR + SUFFIX_3D_DIRS\n",
        "    elif (mode == '2D'):\n",
        "      fileBase = SHUFFLED_DIR + SUFFIX_2D_DIRS\n",
        "    else:\n",
        "      raise Exception(\"Invalid Data Dimension, please use '3D' or '2D'.\")\n",
        "    self.shuffled_data = {}\n",
        "    fileSuffD = \"D_shuffled.npy\"\n",
        "    fileSuffK = \"K_shuffled.npy\"\n",
        "    for split_num, split_type in enumerate(SPLIT_TYPES):\n",
        "      self.shuffled_data[split_type] = {}\n",
        "      for subject in range(1, NUM_SUBJECTS+1):\n",
        "        self.__liveProgress(\"Loading Shuffled Data\", (split_num)*NUM_SUBJECTS + subject, NUM_SUBJECTS*len(SPLIT_TYPES))\n",
        "        d = np.load(fileBase + SPLIT_DIR_DICT[split_type] + \"A0\" + str(subject) + fileSuffD)\n",
        "        k = np.load(fileBase + SPLIT_DIR_DICT[split_type] + \"A0\" + str(subject) + fileSuffK)\n",
        "        self.shuffled_data[split_type][subject] = (d, k)\n",
        "    print(\"\\rFinished loading shuffled data\")\n",
        "\n",
        "  def getProcessedData(self, split_type, subject):\n",
        "    return self.processed_data[split_type][subject][0], self.processed_data[split_type][subject][1]\n",
        "\n",
        "  def getCroppedData(self, split_type, subject):\n",
        "    return self.cropped_data[split_type][subject][0], self.cropped_data[split_type][subject][1]\n",
        "\n",
        "  def getAveragedData(self, split_type, subject):  \n",
        "    return self.averaged_data[split_type][subject][0], self.averaged_data[split_type][subject][1]\n",
        "\n",
        "  def getShuffledData(self, split_type, subject):\n",
        "    return self.shuffled_data[split_type][subject][0], self.shuffled_data[split_type][subject][1]  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKwgjKyYOtQ-"
      },
      "source": [
        "Process Data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rA3gSjrOub4",
        "outputId": "411a8a62-ede0-4a4f-e2a4-35003b0689f6"
      },
      "source": [
        "processor = DataProcessing(mode=MODE)\n",
        "\n",
        "processor.processData()\n",
        "\n",
        "processor.saveProcessedData()\n",
        "\n",
        "del(processor)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 9 Training Files\n",
            "Found 9 Evaluation Files\n",
            "\rProcessing Data - [=-----------------] - 1/18 - 5.56%Filtering raw data in 1 contiguous segment\n",
            "Setting up band-pass filter from 4 - 40 Hz\n",
            "\n",
            "FIR filter parameters\n",
            "---------------------\n",
            "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
            "- Windowed time-domain design (firwin) method\n",
            "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
            "- Lower passband edge: 4.00\n",
            "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 3.00 Hz)\n",
            "- Upper passband edge: 40.00 Hz\n",
            "- Upper transition bandwidth: 10.00 Hz (-6 dB cutoff frequency: 45.00 Hz)\n",
            "- Filter length: 413 samples (1.652 sec)\n",
            "\n",
            "Processing Data - [==----------------] - 2/18 - 11.11%Filtering raw data in 1 contiguous segment\n",
            "Setting up band-pass filter from 4 - 40 Hz\n",
            "\n",
            "FIR filter parameters\n",
            "---------------------\n",
            "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
            "- Windowed time-domain design (firwin) method\n",
            "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
            "- Lower passband edge: 4.00\n",
            "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 3.00 Hz)\n",
            "- Upper passband edge: 40.00 Hz\n",
            "- Upper transition bandwidth: 10.00 Hz (-6 dB cutoff frequency: 45.00 Hz)\n",
            "- Filter length: 413 samples (1.652 sec)\n",
            "\n",
            "Processing Data - [===---------------] - 3/18 - 16.67%Filtering raw data in 1 contiguous segment\n",
            "Setting up band-pass filter from 4 - 40 Hz\n",
            "\n",
            "FIR filter parameters\n",
            "---------------------\n",
            "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
            "- Windowed time-domain design (firwin) method\n",
            "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
            "- Lower passband edge: 4.00\n",
            "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 3.00 Hz)\n",
            "- Upper passband edge: 40.00 Hz\n",
            "- Upper transition bandwidth: 10.00 Hz (-6 dB cutoff frequency: 45.00 Hz)\n",
            "- Filter length: 413 samples (1.652 sec)\n",
            "\n",
            "Processing Data - [====--------------] - 4/18 - 22.22%Filtering raw data in 1 contiguous segment\n",
            "Setting up band-pass filter from 4 - 40 Hz\n",
            "\n",
            "FIR filter parameters\n",
            "---------------------\n",
            "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
            "- Windowed time-domain design (firwin) method\n",
            "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
            "- Lower passband edge: 4.00\n",
            "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 3.00 Hz)\n",
            "- Upper passband edge: 40.00 Hz\n",
            "- Upper transition bandwidth: 10.00 Hz (-6 dB cutoff frequency: 45.00 Hz)\n",
            "- Filter length: 413 samples (1.652 sec)\n",
            "\n",
            "Processing Data - [=====-------------] - 5/18 - 27.78%Filtering raw data in 1 contiguous segment\n",
            "Setting up band-pass filter from 4 - 40 Hz\n",
            "\n",
            "FIR filter parameters\n",
            "---------------------\n",
            "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
            "- Windowed time-domain design (firwin) method\n",
            "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
            "- Lower passband edge: 4.00\n",
            "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 3.00 Hz)\n",
            "- Upper passband edge: 40.00 Hz\n",
            "- Upper transition bandwidth: 10.00 Hz (-6 dB cutoff frequency: 45.00 Hz)\n",
            "- Filter length: 413 samples (1.652 sec)\n",
            "\n",
            "Processing Data - [======------------] - 6/18 - 33.33%Filtering raw data in 1 contiguous segment\n",
            "Setting up band-pass filter from 4 - 40 Hz\n",
            "\n",
            "FIR filter parameters\n",
            "---------------------\n",
            "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
            "- Windowed time-domain design (firwin) method\n",
            "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
            "- Lower passband edge: 4.00\n",
            "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 3.00 Hz)\n",
            "- Upper passband edge: 40.00 Hz\n",
            "- Upper transition bandwidth: 10.00 Hz (-6 dB cutoff frequency: 45.00 Hz)\n",
            "- Filter length: 413 samples (1.652 sec)\n",
            "\n",
            "Processing Data - [=======-----------] - 7/18 - 38.89%Filtering raw data in 1 contiguous segment\n",
            "Setting up band-pass filter from 4 - 40 Hz\n",
            "\n",
            "FIR filter parameters\n",
            "---------------------\n",
            "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
            "- Windowed time-domain design (firwin) method\n",
            "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
            "- Lower passband edge: 4.00\n",
            "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 3.00 Hz)\n",
            "- Upper passband edge: 40.00 Hz\n",
            "- Upper transition bandwidth: 10.00 Hz (-6 dB cutoff frequency: 45.00 Hz)\n",
            "- Filter length: 413 samples (1.652 sec)\n",
            "\n",
            "Processing Data - [========----------] - 8/18 - 44.44%Filtering raw data in 1 contiguous segment\n",
            "Setting up band-pass filter from 4 - 40 Hz\n",
            "\n",
            "FIR filter parameters\n",
            "---------------------\n",
            "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
            "- Windowed time-domain design (firwin) method\n",
            "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
            "- Lower passband edge: 4.00\n",
            "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 3.00 Hz)\n",
            "- Upper passband edge: 40.00 Hz\n",
            "- Upper transition bandwidth: 10.00 Hz (-6 dB cutoff frequency: 45.00 Hz)\n",
            "- Filter length: 413 samples (1.652 sec)\n",
            "\n",
            "Processing Data - [=========---------] - 9/18 - 50.00%Filtering raw data in 1 contiguous segment\n",
            "Setting up band-pass filter from 4 - 40 Hz\n",
            "\n",
            "FIR filter parameters\n",
            "---------------------\n",
            "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
            "- Windowed time-domain design (firwin) method\n",
            "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
            "- Lower passband edge: 4.00\n",
            "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 3.00 Hz)\n",
            "- Upper passband edge: 40.00 Hz\n",
            "- Upper transition bandwidth: 10.00 Hz (-6 dB cutoff frequency: 45.00 Hz)\n",
            "- Filter length: 413 samples (1.652 sec)\n",
            "\n",
            "Processing Data - [==========--------] - 10/18 - 55.56%Filtering raw data in 1 contiguous segment\n",
            "Setting up band-pass filter from 4 - 40 Hz\n",
            "\n",
            "FIR filter parameters\n",
            "---------------------\n",
            "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
            "- Windowed time-domain design (firwin) method\n",
            "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
            "- Lower passband edge: 4.00\n",
            "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 3.00 Hz)\n",
            "- Upper passband edge: 40.00 Hz\n",
            "- Upper transition bandwidth: 10.00 Hz (-6 dB cutoff frequency: 45.00 Hz)\n",
            "- Filter length: 413 samples (1.652 sec)\n",
            "\n",
            "Processing Data - [===========-------] - 11/18 - 61.11%Filtering raw data in 1 contiguous segment\n",
            "Setting up band-pass filter from 4 - 40 Hz\n",
            "\n",
            "FIR filter parameters\n",
            "---------------------\n",
            "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
            "- Windowed time-domain design (firwin) method\n",
            "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
            "- Lower passband edge: 4.00\n",
            "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 3.00 Hz)\n",
            "- Upper passband edge: 40.00 Hz\n",
            "- Upper transition bandwidth: 10.00 Hz (-6 dB cutoff frequency: 45.00 Hz)\n",
            "- Filter length: 413 samples (1.652 sec)\n",
            "\n",
            "Processing Data - [============------] - 12/18 - 66.67%Filtering raw data in 1 contiguous segment\n",
            "Setting up band-pass filter from 4 - 40 Hz\n",
            "\n",
            "FIR filter parameters\n",
            "---------------------\n",
            "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
            "- Windowed time-domain design (firwin) method\n",
            "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
            "- Lower passband edge: 4.00\n",
            "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 3.00 Hz)\n",
            "- Upper passband edge: 40.00 Hz\n",
            "- Upper transition bandwidth: 10.00 Hz (-6 dB cutoff frequency: 45.00 Hz)\n",
            "- Filter length: 413 samples (1.652 sec)\n",
            "\n",
            "Processing Data - [=============-----] - 13/18 - 72.22%Filtering raw data in 1 contiguous segment\n",
            "Setting up band-pass filter from 4 - 40 Hz\n",
            "\n",
            "FIR filter parameters\n",
            "---------------------\n",
            "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
            "- Windowed time-domain design (firwin) method\n",
            "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
            "- Lower passband edge: 4.00\n",
            "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 3.00 Hz)\n",
            "- Upper passband edge: 40.00 Hz\n",
            "- Upper transition bandwidth: 10.00 Hz (-6 dB cutoff frequency: 45.00 Hz)\n",
            "- Filter length: 413 samples (1.652 sec)\n",
            "\n",
            "Processing Data - [==============----] - 14/18 - 77.78%Filtering raw data in 1 contiguous segment\n",
            "Setting up band-pass filter from 4 - 40 Hz\n",
            "\n",
            "FIR filter parameters\n",
            "---------------------\n",
            "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
            "- Windowed time-domain design (firwin) method\n",
            "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
            "- Lower passband edge: 4.00\n",
            "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 3.00 Hz)\n",
            "- Upper passband edge: 40.00 Hz\n",
            "- Upper transition bandwidth: 10.00 Hz (-6 dB cutoff frequency: 45.00 Hz)\n",
            "- Filter length: 413 samples (1.652 sec)\n",
            "\n",
            "Processing Data - [===============---] - 15/18 - 83.33%Filtering raw data in 1 contiguous segment\n",
            "Setting up band-pass filter from 4 - 40 Hz\n",
            "\n",
            "FIR filter parameters\n",
            "---------------------\n",
            "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
            "- Windowed time-domain design (firwin) method\n",
            "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
            "- Lower passband edge: 4.00\n",
            "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 3.00 Hz)\n",
            "- Upper passband edge: 40.00 Hz\n",
            "- Upper transition bandwidth: 10.00 Hz (-6 dB cutoff frequency: 45.00 Hz)\n",
            "- Filter length: 413 samples (1.652 sec)\n",
            "\n",
            "Processing Data - [================--] - 16/18 - 88.89%Filtering raw data in 1 contiguous segment\n",
            "Setting up band-pass filter from 4 - 40 Hz\n",
            "\n",
            "FIR filter parameters\n",
            "---------------------\n",
            "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
            "- Windowed time-domain design (firwin) method\n",
            "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
            "- Lower passband edge: 4.00\n",
            "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 3.00 Hz)\n",
            "- Upper passband edge: 40.00 Hz\n",
            "- Upper transition bandwidth: 10.00 Hz (-6 dB cutoff frequency: 45.00 Hz)\n",
            "- Filter length: 413 samples (1.652 sec)\n",
            "\n",
            "Processing Data - [=================-] - 17/18 - 94.44%Filtering raw data in 1 contiguous segment\n",
            "Setting up band-pass filter from 4 - 40 Hz\n",
            "\n",
            "FIR filter parameters\n",
            "---------------------\n",
            "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
            "- Windowed time-domain design (firwin) method\n",
            "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
            "- Lower passband edge: 4.00\n",
            "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 3.00 Hz)\n",
            "- Upper passband edge: 40.00 Hz\n",
            "- Upper transition bandwidth: 10.00 Hz (-6 dB cutoff frequency: 45.00 Hz)\n",
            "- Filter length: 413 samples (1.652 sec)\n",
            "\n",
            "Processing Data - [==================] - 18/18 - 100.00%Filtering raw data in 1 contiguous segment\n",
            "Setting up band-pass filter from 4 - 40 Hz\n",
            "\n",
            "FIR filter parameters\n",
            "---------------------\n",
            "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
            "- Windowed time-domain design (firwin) method\n",
            "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
            "- Lower passband edge: 4.00\n",
            "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 3.00 Hz)\n",
            "- Upper passband edge: 40.00 Hz\n",
            "- Upper transition bandwidth: 10.00 Hz (-6 dB cutoff frequency: 45.00 Hz)\n",
            "- Filter length: 413 samples (1.652 sec)\n",
            "\n",
            "Finished processing data\n",
            "Finished saving processed data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLYuS3FO7Bvu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0307cf02-7ecc-4491-e197-7959085eded9"
      },
      "source": [
        "processor = DataProcessing(mode=MODE)\n",
        "\n",
        "processor.loadProcessedData(mode=MODE)\n",
        "\n",
        "processor.cropData()\n",
        "\n",
        "processor.saveCroppedData()\n",
        "\n",
        "del(processor)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 9 Training Files\n",
            "Found 9 Evaluation Files\n",
            "Finished loading processed data\n",
            "Finished cropping data\n",
            "Finished saving cropped data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npVV2NqzZIrz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad79c50c-fa2e-4944-b246-34fc20377750"
      },
      "source": [
        "processor = DataProcessing(mode=MODE)\n",
        "\n",
        "processor.loadCroppedData(mode=MODE)\n",
        "\n",
        "processor.averageData()\n",
        "\n",
        "processor.saveAveragedData()\n",
        "\n",
        "del(processor)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 9 Training Files\n",
            "Found 9 Evaluation Files\n",
            "Finished loading cropped data\n",
            "Finished averaging data\n",
            "Finished saving averaged data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZucG5NpOaKHK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49529fc2-4cf1-4d4f-dc8d-271ce5bbe581"
      },
      "source": [
        "processor = DataProcessing(mode=MODE)\n",
        "\n",
        "processor.loadAveragedData(mode=MODE)\n",
        "\n",
        "processor.shuffleData()\n",
        "\n",
        "processor.saveShuffledData()\n",
        "\n",
        "del(processor)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 9 Training Files\n",
            "Found 9 Evaluation Files\n",
            "Finished loading averaged data\n",
            "Finished shuffling data\n",
            "Finished saving shuffled data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rpzLLWvd3Nc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "outputId": "edf227e9-2696-49d8-ded2-7b9a3519be75"
      },
      "source": [
        "processor = DataProcessing(mode=MODE)\n",
        "processor.loadProcessedData(mode=MODE)\n",
        "processor.loadCroppedData(mode=MODE)\n",
        "processor.loadAveragedData(mode=MODE)\n",
        "processor.loadShuffledData(mode=MODE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 9 Training Files\n",
            "Found 9 Evaluation Files\n",
            "Finished loading processed data\n",
            "Finished loading cropped data\n",
            "Loading Averaged Data - [===============------------] - 15/27 - 55.56%"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-9e5f02ef51b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadProcessedData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMODE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadCroppedData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMODE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadAveragedData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMODE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadShuffledData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMODE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-577f31e75616>\u001b[0m in \u001b[0;36mloadAveragedData\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0msubject\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_SUBJECTS\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__liveProgress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading Averaged Data\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msplit_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mNUM_SUBJECTS\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msubject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_SUBJECTS\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSPLIT_TYPES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileBase\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mSPLIT_DIR_DICT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msplit_type\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"A0\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfileSuffD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileBase\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mSPLIT_DIR_DICT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msplit_type\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"A0\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfileSuffK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maveraged_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msplit_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    439\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m                 return format.read_array(fid, allow_pickle=allow_pickle,\n\u001b[0;32m--> 441\u001b[0;31m                                          pickle_kwargs=pickle_kwargs)\n\u001b[0m\u001b[1;32m    442\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m             \u001b[0;31m# Try a pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    755\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;31m# We can use the fast fromfile() function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0;31m# This is not a real file. We have to read it the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hP1t244tgoYL"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_label_distribution(Y):\n",
        "  counts = sum(Y)\n",
        "  x_labels = [str(val) for val in range(NUM_CLASSES)]\n",
        "\n",
        "  plt.bar(x_labels, counts)  # arguments are passed to plt.bar\n",
        "  plt.title(\"Y Label Distribution\")\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inspecting Data"
      ],
      "metadata": {
        "id": "I5fLYHCPBGBd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inspecting Processed Data"
      ],
      "metadata": {
        "id": "s3jic2vIBDJy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d, k = processor.getProcessedData('val', 1)\n",
        "\n",
        "print(\"X Data Shape: \")\n",
        "print(d.shape)\n",
        "print(\"Y Data Shape: \")\n",
        "print(k.shape)\n",
        "print(\"Label Distribution: \")\n",
        "print(sum(k))\n",
        "\n",
        "plot_label_distribution(k)\n",
        "\n",
        "N = 5\n",
        "print(\"First {} Labels\".format(N))\n",
        "print(k[:N])"
      ],
      "metadata": {
        "id": "psVw_TgwBAWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inspecting Cropped Data"
      ],
      "metadata": {
        "id": "XuqsJrJoN0j3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d, k = processor.getCroppedData('val', 1)\n",
        "\n",
        "print(\"X Data Shape: \")\n",
        "print(d.shape)\n",
        "print(\"Y Data Shape: \")\n",
        "print(k.shape)\n",
        "print(\"Label Distribution: \")\n",
        "print(sum(k))\n",
        "\n",
        "plot_label_distribution(k)\n",
        "\n",
        "N = 5\n",
        "print(\"First {} Labels\".format(N))\n",
        "print(k[:N])"
      ],
      "metadata": {
        "id": "pWUc4d5BN2V2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inspecting Averaged Data"
      ],
      "metadata": {
        "id": "c8pf1QkJN4W-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d, k = processor.getAveragedData('train', 1)\n",
        "\n",
        "print(\"X Data Shape: \")\n",
        "print(d.shape)\n",
        "print(\"Y Data Shape: \")\n",
        "print(k.shape)\n",
        "print(\"Label Distribution: \")\n",
        "print(sum(k))\n",
        "\n",
        "plot_label_distribution(k)\n",
        "\n",
        "N = 5\n",
        "print(\"First {} Labels\".format(N))\n",
        "print(k[:N])"
      ],
      "metadata": {
        "id": "g6iuL5oPN55U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inspecting Shuffled Data"
      ],
      "metadata": {
        "id": "z-Y_DvUIBIBN"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eo0_A99TecUR"
      },
      "source": [
        "d, k = processor.getShuffledData('train', 1)\n",
        "\n",
        "print(\"X Data Shape: \")\n",
        "print(d.shape)\n",
        "print(\"Y Data Shape: \")\n",
        "print(k.shape)\n",
        "print(\"Label Distribution: \")\n",
        "print(sum(k))\n",
        "\n",
        "plot_label_distribution(k)\n",
        "\n",
        "N = 5\n",
        "print(\"First {} Labels\".format(N))\n",
        "print(k[:N])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}